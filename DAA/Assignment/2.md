# Assignment 2

## 1. Explain greedy and dynamic programming approaches in analysis of algorithms.

Certainly, Dev. Let's delve into the explanations of greedy and dynamic programming approaches in the analysis of algorithms.

### Greedy Algorithm:
A greedy algorithm makes locally optimal choices at each stage with the hope of finding a global optimum. In other words, it chooses the best option at the current moment without considering the consequences of that choice on future steps. Greedy algorithms are often used for optimization problems.

Example:
- **Activity Selection:** Consider a set of activities each marked with a start and finish time. The goal is to select the maximum number of non-overlapping activities.

### Dynamic Programming:
Dynamic programming is a method for efficiently solving a broad range of search and optimization problems that exhibit the property of overlapping subproblems and optimal substructure. It involves breaking down a problem into smaller overlapping subproblems and solving each subproblem only once, storing the solutions to subproblems in a table to avoid redundant computations.

Example:
- **Fibonacci Sequence:** Instead of recursively computing Fibonacci numbers, dynamic programming would store the results of previous calculations, preventing repetitive computations and improving efficiency.

### Key Differences:

1. **Optimality:**
   - Greedy: Makes locally optimal choices with the hope of finding a global optimum.
   - Dynamic Programming: Solves subproblems and builds up to an optimal solution.

2. **Approach:**
   - Greedy: Top-down, making choices without considering future consequences.
   - Dynamic Programming: Bottom-up, solving subproblems and combining solutions.

3. **Memory Usage:**
   - Greedy: Generally requires less memory.
   - Dynamic Programming: Involves storing solutions to subproblems, often using a table or memoization.

4. **Backtracking:**
   - Greedy: Typically does not involve backtracking.
   - Dynamic Programming: Can involve backtracking when constructing the optimal solution.

### Conclusion:
In summary, both greedy and dynamic programming are powerful algorithmic paradigms, each with its strengths and applications. Greedy algorithms are simpler and often faster but may not always lead to globally optimal solutions. Dynamic programming, on the other hand, guarantees optimality but can be more memory-intensive.

Feel free to ask if you have further questions or need clarification on specific aspects, Dev.

## 2. What is divide and conquer strategy? Write an algorithm for quick sort

Divide and conquer is a problem-solving strategy in computer science where a problem is broken down into smaller sub-problems, which are solved independently. The solutions to the sub-problems are then combined to solve the original problem. This approach is often used to make complex problems more manageable.

As for the QuickSort algorithm, it's a classic example of a divide-and-conquer algorithm used for sorting. Here's a simplified version in pseudocode:

```plaintext
function quicksort(arr):
    if length(arr) <= 1:
        return arr
    else:
        pivot = arr[0]
        less = [x for x in arr[1:] if x <= pivot]
        greater = [x for x in arr[1:] if x > pivot]
        return quicksort(less) + [pivot] + quicksort(greater)
```

This algorithm works by selecting a pivot element from the array and partitioning the other elements into two sub-arrays according to whether they are less than or greater than the pivot. The process is then applied recursively to the sub-arrays.

Keep in mind that this is a basic version, and there are more optimized implementations, especially for large datasets. Additionally, the choice of the pivot can impact the efficiency of the algorithm, and various strategies exist for selecting it.

## 3. Explain the merge sort algorithm giving an example. Discuss its complexity

Certainly, Dev. Merge Sort is a divide-and-conquer algorithm that works by recursively dividing the input array into smaller halves, sorting them, and then merging them back together. Here's a step-by-step explanation:

1. **Divide:** Split the unsorted array into two halves.
2. **Conquer:** Recursively sort each half.
3. **Merge:** Merge the sorted halves to produce a single sorted array.

Let's consider an example with an array `[38, 27, 43, 3, 9, 82, 10]`:

1. **Initial array:** `[38, 27, 43, 3, 9, 82, 10]`
2. **Divide:** Split into `[38, 27, 43]` and `[3, 9, 82, 10]`.
3. **Conquer:** Continue dividing and sorting each sub-array.
   - `[38, 27, 43]` becomes `[27, 38, 43]`.
   - `[3, 9, 82, 10]` becomes `[3, 9]` and `[10, 82]`.
4. **Merge:** Merge the sorted sub-arrays.
   - Merge `[27, 38, 43]` with `[3, 9, 10, 82]`.

The final sorted array is `[3, 9, 10, 27, 38, 43, 82]`.

**Complexity:**

- **Time Complexity:** Merge Sort has a time complexity of O(n log n), where n is the number of elements in the array. This makes it efficient for large datasets.
- **Space Complexity:** The space complexity is O(n) because it requires additional space for the temporary arrays during the merge phase.

The key advantage of Merge Sort is its consistent O(n log n) performance, making it suitable for large-scale data sets. However, the drawback is its space complexity, especially in situations with limited memory.

## 4. Solve the following recurrence relation: $T(n) = 1$ if $n = 1$ and $T(n) = 2T(n-1)$ if $n > 1$

## 5. For the given set of items and knapsack capacity of 10kg, find the subset of the items to be added in the knapsack such that the profit is maximum. $W_i = {3, 4, 2, 5, 1}$, $P_i = {10, 15, 10, 12, 8}$

## 6. Find the minimum cost spanning tree

```dev
 - AB - 6
 - AG - 15
 - BC - 11
 - BD - 5
 - CG - 25
 - CF - 9
 - GF - 12
 - CD - 17
 - DE - 22
 - EF - 10
```
